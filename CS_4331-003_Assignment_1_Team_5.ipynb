{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ae7efb-f21e-41de-bc4b-3a24bb23ee48",
   "metadata": {},
   "source": [
    "# Team Name: Red Shufflers\n",
    "\n",
    "Team Members: Samuel Duro-Aina, Thomas Gilmore, Wally Alli, Matthew Ojeda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8374c93-578a-47f8-a078-b563284b637f",
   "metadata": {},
   "source": [
    "# Deep Learning Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b53b0-676b-40b8-aebd-8e619fe0faa5",
   "metadata": {},
   "source": [
    "Given a state for the game of Durak we want to find out what the most optimal next play is. By stringing the most optimal plays together the goal is to to create an AI that can play a perfect game of Durak against either people or other AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09221f6-a202-4df4-9fc7-5470c9f0ba92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overview of Past and Current Solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4643a-e696-4dcf-8a58-8b3e7d130181",
   "metadata": {},
   "source": [
    "The idea of implementing AI in order to be able to play games or assist in the creation of games is an old concepts. Since the development of AI, there has been an effort in order to integrate them within games or to leverage games in order to improve AI. Games offer computer scientist and programmer a great enviorment to test and explore the capabilities of AI.\n",
    "Whether it be from the contrained enviorment, a fixed set of rules, and knowing the results and the required inputs to obtain the results, gaming has been a great way to develop AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc0a59-1151-4341-b1b6-1d17ad5a62d2",
   "metadata": {},
   "source": [
    "### Article 1\n",
    "Alot of Ideas and solutions have been founded or utiilize in order to integrate AI into games. Ideas that we may be potentially interested in utilizing in order to make our own system and program more efficient and reliable\n",
    "Reinforcement Learning is an example. \n",
    "We found an article, A Review of Real-Time Strategy Game AI, that talked about how to integerate AI into Real Time Strategy Games, Such as Starcraft. They tried using many techniques such as Goal Driven Autonomy, Reinforcement learning, and Game tree search. Ideas that we may be interested in implementing ourselves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369973bc-e059-486c-a4e6-3a8d9ccd9448",
   "metadata": {},
   "source": [
    "### Article 2\n",
    "Another Solution/Feature we discovered through research in another article, Creating Pro-Level AI for a Real Time fighting Game using Deep Reinforcement Learning, is the idea of reward shaping. \n",
    "They wanted to create an AI for a real time fighting game using deep reinforcement learning and had implemented reward shaping.\n",
    "we noted this because its another idea in order to decide on optimal ways for our agent to be able to select the best option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf268ba-2fe6-44cd-9a69-4c31b565457e",
   "metadata": {},
   "source": [
    "### Artice 3\n",
    "Within M. Colledanchise and P. Ögren, \"How Behavior Trees Modularize Hybrid Control Systems and Generalize Sequential Behavior Compositions, the Subsumption Architecture, and Decision Trees,\" we also found the idea of Descison trees.\n",
    "    \n",
    "    Descision trees are tree that aggregate a number of If clauses, that leads to a given decision or prediction. Each leaf of the tree represents a particular decision, prediction, conclusion, or action to be carried out, and each nonleaf represent a predicate to be checked. \n",
    "This can be used to have the Agent take actions based on the actions of the Player it will be playing against.\n",
    "There for allowing the Agent to react and act upon the game in reaction to the changes in the game as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c59537-9228-439e-9f58-ee31eafa4354",
   "metadata": {},
   "source": [
    "### Article 4\n",
    "To expand on the Idea of using descision trees in, \"Emulating Human Play in a Leading Mobile Card Game\", we learned about what seems to be a more specific version of descision trees Monte Carlo Tree Search.\n",
    "Monte Carlo Tree Search or commonly known as MCTS creates an asymmetrical decision tree, with specific focus on exploring the more promising decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99a42c-cad3-44c5-98b9-908fe9684382",
   "metadata": {},
   "source": [
    "### Article 5\n",
    "The last idea that we consider while being a basic concept felt important to consider and learn because of how consistently the method has been implemented with AI and games, MinMax. In \"Hybrid Optimization for Developing Human Like Chess Playing System,\"we learned about how this technique has been implemented in the AI that is used for multiple games such as Chess. This is implemented to find the best moves to play in the case the opponent is playing optimally. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4fc28-2796-42de-99f3-ec5ec67b38f3",
   "metadata": {},
   "source": [
    "The reason why all of these ideas and solutions could potentially be important to our project is because of the manner of our game. Its a game where sometimes the best action is to lose a turn or multiple turns in order to win the entire game. There are many factors to consider each turn that the model must be able to handle in order to win games. We will do our best to consider all of these solutions and prepare our model for different use cases that could occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd2883-ed1d-41f1-8199-ea948d3fadd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Our Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628193e-2ff3-4808-bd2b-30f200d3bc14",
   "metadata": {},
   "source": [
    "The way we decided to to train the Model against itself and use the Monte Carlo tree search (MCTS) in a nondeterministic way. The model will start by playing randomly and will be rewarded when ever it wins. We would also reward the Model whenever it made a valuable trade in resources (example being defending a 6 with 7 rather than queen). Because of its rewards the MCTS will now make the Model more likely to choose the better options.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc7fb8e-1dc8-48c7-b4ba-7c886bb7fd1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hardware Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee782d41-6342-42d3-827a-315805449875",
   "metadata": {},
   "source": [
    "•\tWindows 10\n",
    "\n",
    "•\tIntel Core i7 6 core or higher core processor\n",
    "\n",
    "•\tMinimum 16GB DDR4/DDR5 RAM\n",
    "\n",
    "•\t512GB Solid State Drive (SSD)(Primary drive for operating system and software)\n",
    "\n",
    "\n",
    "•\tDedicated GPU: RTX A2000 - 4000 or GeForce RTX 3060 - 3080\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b9507-7ff2-4b00-b5d5-919b49073674",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Software Requirments \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde41261-6784-4de1-930c-5211d4422d0f",
   "metadata": {},
   "source": [
    "•\tPython 3, TensorFlow, Keras\n",
    "\n",
    "•\tJupyterlab, JupyterNotebook\n",
    "\n",
    "•\tAnaconda Navigator\n",
    "\n",
    "•\tVisual Studio Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e73d2b-e60b-4f1a-b170-179211c0620c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Requirments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2054e-d2e0-489e-a641-0515764e7043",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Game State</li>\n",
    "<li>Card rankings and suits (trump card)</li>\n",
    "<li>Cards left in the deck</li>\n",
    "<li>Cards that have been discarded</li>\n",
    "<li>Cards that are on the play field</li>\n",
    "<li>Cards in the agent's hand</li>\n",
    "<li>Cards that have been taken by opponent</li>\n",
    "<li>Records from previous games/rounds</li>\n",
    "<li>Records from simulated games</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4e59a-cea1-44bd-ae82-74f8bf13a061",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f875bb-45e8-44be-b3a7-667ec137b1e0",
   "metadata": {},
   "source": [
    "|Group Member|Work to be Accomplished| \n",
    "|:-----------|:----------------------|\n",
    "|Wally Alli|Model Develpoment|\n",
    "|Samuel Duro-Aina|Model Develpoment|\n",
    "|Thomas Gilmore|Non Visual- Durak Game (for optimization)|\n",
    "|Matthew Ojeda|Training Set|\n",
    "|TBA|(Optional) GUI representaion of simulated game|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c8289-aac3-4eed-8268-7f9109c2843d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Contributions so Far\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111453eb-885d-4b0b-92a8-ff57e77d8109",
   "metadata": {},
   "source": [
    "|Group Member|Contribution for Part 1| \n",
    "|:-----------|:----------------------|\n",
    "|Wally Alli|Researched Deep Learning Articles|\n",
    "|Samuel Duro-Aina|Checked minimum Hardware & Software Requirements|\n",
    "|Thomas Gilmore|Developed Hybrid Learning System|\n",
    "|Matthew Ojeda|Analyzed Important Data|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42adf23",
   "metadata": {},
   "source": [
    "#  Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18e5f1",
   "metadata": {},
   "source": [
    "V. Chole and V. Gadicha, \"Hybrid Optimization for Developing Human Like Chess Playing System,\" 2022 IEEE 3rd Global Conference for Advancement in Technology (GCAT), Bangalore, India, 2022, pp. 1-5, doi: 10.1109/GCAT55367.2022.9971825.\n",
    "\n",
    "H. Baier, A. Sattaur, E. J. Powley, S. Devlin, J. Rollason and P. I. Cowling, \"Emulating Human Play in a Leading Mobile Card Game,\" in IEEE Transactions on Games, vol. 11, no. 4, pp. 386-395, Dec. 2019, doi: 10.1109/TG.2018.2835764.\n",
    "\n",
    "M. Colledanchise and P. Ögren, \"How Behavior Trees Modularize Hybrid Control Systems and Generalize Sequential Behavior Compositions, the Subsumption Architecture, and Decision Trees,\" in IEEE Transactions on Robotics, vol. 33, no. 2, pp. 372-389, April 2017, doi: 10.1109/TRO.2016.2633567.\n",
    "\n",
    "I. Oh, S. Rho, S. Moon, S. Son, H. Lee and J. Chung, \"Creating Pro-Level AI for a Real-Time Fighting Game Using Deep Reinforcement Learning,\" in IEEE Transactions on Games, vol. 14, no. 2, pp. 212-220, June 2022, doi: 10.1109/TG.2021.3049539.\n",
    "\n",
    "Robertson, G. and Watson, I. (2014), A Review of Real-Time Strategy Game AI. AI Magazine, 35: 75-104. https://doi.org/10.1609/aimag.v35i4.2478\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e2c48-714c-4180-8d37-9f2d1b526600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
